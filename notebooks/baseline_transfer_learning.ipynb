{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, pipeline, BertTokenizer, BertModel\n",
    "\n",
    "# Specifying device (CPU if GPU not compatible with CUDA)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>amir</th>\n",
       "      <th>charles</th>\n",
       "      <th>moindze</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>label</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>Près de cent mille euros d'impôts… Délire tota...</td>\n",
       "      <td>0</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>dissident</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>8</td>\n",
       "      <td>UHJvcG9zYWw6MTI5MGExOTYtMWUzZS0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>465</td>\n",
       "      <td>Injuste et inefficace</td>\n",
       "      <td>1</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>0</td>\n",
       "      <td>UHJvcG9zYWw6ODRjZTk3NmYtMWUyYy0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466</td>\n",
       "      <td>pitoyable</td>\n",
       "      <td>2</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>dissident</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>0</td>\n",
       "      <td>UHJvcG9zYWw6OTBhZmQ5YWUtMWUyZS0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467</td>\n",
       "      <td>Ils ne peuvent ni revendiquer, ni manifester, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>dissident</td>\n",
       "      <td>avis acceptable</td>\n",
       "      <td>inclassable</td>\n",
       "      <td>8</td>\n",
       "      <td>UHJvcG9zYWw6M2E1MzhkZWQtMWUyYy0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>468</td>\n",
       "      <td>Suppressions de toutes les agences dites ‘’d’u...</td>\n",
       "      <td>5</td>\n",
       "      <td>avis acceptable</td>\n",
       "      <td>avis acceptable</td>\n",
       "      <td>avis acceptable</td>\n",
       "      <td>19</td>\n",
       "      <td>UHJvcG9zYWw6OTgxMjAyOGItMWUyZi0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>1576</td>\n",
       "      <td>C'est trop compliqué avec trop de services en ...</td>\n",
       "      <td>21704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>UHJvcG9zYWw6Y2NkMWFiYjctMWY1My0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>0.256870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1577</td>\n",
       "      <td>Maintenir et favoriser l'enseignement des lang...</td>\n",
       "      <td>77105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>UHJvcG9zYWw6Y2VhN2IzN2ItMjcwOC0xMWU5LTk0ZDItZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>0.255510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>1578</td>\n",
       "      <td>Donc une redéfinition claire pour tous de ce q...</td>\n",
       "      <td>99216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>UHJvcG9zYWw6MjlmNWExNDgtMmM1NS0xMWU5LWJmNTYtZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>0.255188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>1579</td>\n",
       "      <td>trop de strates et de niches mises en place pa...</td>\n",
       "      <td>188706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>UHJvcG9zYWw6NTJmZDVlMzgtNDMyZS0xMWU5LWJmNTYtZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>0.254651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>1580</td>\n",
       "      <td>C'est une garantie de sécurité pour chaque cit...</td>\n",
       "      <td>112912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>UHJvcG9zYWw6NmNhNjg0ZTgtMzBhNi0xMWU5LWJmNTYtZm...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>0.254623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1078 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  Unnamed: 2  \\\n",
       "0      464  Près de cent mille euros d'impôts… Délire tota...           0   \n",
       "1      465                              Injuste et inefficace           1   \n",
       "2      466                                          pitoyable           2   \n",
       "3      467  Ils ne peuvent ni revendiquer, ni manifester, ...           3   \n",
       "4      468  Suppressions de toutes les agences dites ‘’d’u...           5   \n",
       "...    ...                                                ...         ...   \n",
       "1073  1576  C'est trop compliqué avec trop de services en ...       21704   \n",
       "1074  1577  Maintenir et favoriser l'enseignement des lang...       77105   \n",
       "1075  1578  Donc une redéfinition claire pour tous de ce q...       99216   \n",
       "1076  1579  trop de strates et de niches mises en place pa...      188706   \n",
       "1077  1580  C'est une garantie de sécurité pour chaque cit...      112912   \n",
       "\n",
       "                 amir          charles          moindze  sent_id  \\\n",
       "0         inclassable        dissident      inclassable        8   \n",
       "1         inclassable        dissident        dissident        0   \n",
       "2         inclassable        dissident      inclassable        0   \n",
       "3           dissident  avis acceptable      inclassable        8   \n",
       "4     avis acceptable  avis acceptable  avis acceptable       19   \n",
       "...               ...              ...              ...      ...   \n",
       "1073              NaN              NaN              NaN        0   \n",
       "1074              NaN              NaN              NaN        0   \n",
       "1075              NaN              NaN              NaN        6   \n",
       "1076              NaN              NaN              NaN        0   \n",
       "1077              NaN              NaN              NaN        1   \n",
       "\n",
       "                                            question_id          label  \\\n",
       "0     UHJvcG9zYWw6MTI5MGExOTYtMWUzZS0xMWU5LTk0ZDItZm...      dissident   \n",
       "1     UHJvcG9zYWw6ODRjZTk3NmYtMWUyYy0xMWU5LTk0ZDItZm...      dissident   \n",
       "2     UHJvcG9zYWw6OTBhZmQ5YWUtMWUyZS0xMWU5LTk0ZDItZm...      dissident   \n",
       "3     UHJvcG9zYWw6M2E1MzhkZWQtMWUyYy0xMWU5LTk0ZDItZm...  non dissident   \n",
       "4     UHJvcG9zYWw6OTgxMjAyOGItMWUyZi0xMWU5LTk0ZDItZm...  non dissident   \n",
       "...                                                 ...            ...   \n",
       "1073  UHJvcG9zYWw6Y2NkMWFiYjctMWY1My0xMWU5LTk0ZDItZm...  non dissident   \n",
       "1074  UHJvcG9zYWw6Y2VhN2IzN2ItMjcwOC0xMWU5LTk0ZDItZm...  non dissident   \n",
       "1075  UHJvcG9zYWw6MjlmNWExNDgtMmM1NS0xMWU5LWJmNTYtZm...  non dissident   \n",
       "1076  UHJvcG9zYWw6NTJmZDVlMzgtNDMyZS0xMWU5LWJmNTYtZm...  non dissident   \n",
       "1077  UHJvcG9zYWw6NmNhNjg0ZTgtMzBhNi0xMWU5LWJmNTYtZm...  non dissident   \n",
       "\n",
       "      similarity  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "1073    0.256870  \n",
       "1074    0.255510  \n",
       "1075    0.255188  \n",
       "1076    0.254651  \n",
       "1077    0.254623  \n",
       "\n",
       "[1078 rows x 10 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Près de cent mille euros d'impôts… Délire tota...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Injuste et inefficace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pitoyable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ils ne peuvent ni revendiquer, ni manifester, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suppressions de toutes les agences dites ‘’d’u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>C'est trop compliqué avec trop de services en ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>Maintenir et favoriser l'enseignement des lang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Donc une redéfinition claire pour tous de ce q...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>trop de strates et de niches mises en place pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>C'est une garantie de sécurité pour chaque cit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1078 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Près de cent mille euros d'impôts… Délire tota...      0\n",
       "1                                 Injuste et inefficace      0\n",
       "2                                             pitoyable      0\n",
       "3     Ils ne peuvent ni revendiquer, ni manifester, ...      1\n",
       "4     Suppressions de toutes les agences dites ‘’d’u...      1\n",
       "...                                                 ...    ...\n",
       "1073  C'est trop compliqué avec trop de services en ...      1\n",
       "1074  Maintenir et favoriser l'enseignement des lang...      1\n",
       "1075  Donc une redéfinition claire pour tous de ce q...      1\n",
       "1076  trop de strates et de niches mises en place pa...      1\n",
       "1077  C'est une garantie de sécurité pour chaque cit...      1\n",
       "\n",
       "[1078 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete useless columns\n",
    "dataset = dataset.drop(columns = [\"id\", \"Unnamed: 2\", \"amir\", \"charles\", \"moindze\", \"sent_id\", \"question_id\", \"similarity\"], axis = 1)\n",
    "\n",
    "# Delete Rows\n",
    "dataset = dataset[dataset['label'] != \"inclassable\"]\n",
    "\n",
    "# Encode labels\n",
    "dataset['label'] = dataset['label'].astype(\"category\")\n",
    "dataset.dtypes\n",
    "dataset['label'] = dataset['label'].cat.codes\n",
    "dataset.head()\n",
    "\n",
    "'''\n",
    "In this little preprocessing, our label column has been encoded, we have 0 and 1 values for respectively \"dissident\" and \"non dissident\"\n",
    "'''\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['text'], dataset['label'], \n",
    "                                                                    random_state=2000, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=dataset['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2000, \n",
    "                                                                test_size=0.3, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 10282, 193, 169, 10231, 12908, 80934, 10112, 19688, 10139, 12639, 10104, 180, 112, 21210, 10131, 10139, 17941, 10107, 102], [101, 157, 30698, 10104, 56896, 117, 95554, 10107, 117, 27830, 11170, 16808, 11855, 102, 0, 0, 0, 0, 0, 0], [101, 12271, 16587, 105881, 10129, 18355, 29069, 10554, 53211, 10801, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ninput_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\\nWe add them to both sequences, and 0 represents the padding token.\\n\\n'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import bert-based pretrained model with the multilingual parameter\n",
    "bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenizer with the multilingual parameter\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# sample data\n",
    "text = [\"Il y a une très mauvaise organisation des services de l'état et des administrations\", \n",
    "        \"Trop de députés, sénateurs, trop couteux\",\n",
    "        \"Supprimer toutes celles ne servant pas\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding = True, truncation = True)\n",
    "\n",
    "# output\n",
    "print(sent_id)\n",
    "\n",
    "'''\n",
    "input_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\n",
    "We add them to both sequences, and 0 represents the padding token.\n",
    "\n",
    "'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BERT_Arch.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\notebooks\\baseline_transfer_learning.ipynb Cellule 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: BERT_Arch.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "'''encoded_input = tokenizer(text, return_tensors='pt', padding= True)\n",
    "output = model(**encoded_input)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the sentences.\n",
    "\n",
    "'''\n",
    "With sentences of varying length, we'll use padding to make all the messages havethe same length\n",
    "Before, we'll display the distribution of the sequence lengths in the train set to find the right padding length\n",
    "'''\n",
    "sequence_length = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(sequence_length).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Set 30 as the padding length\n",
    "'''\n",
    "    # tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we can convert sequences to tensors\n",
    "'''\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "\n",
    "# Convert validation set sequences to tensors\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Convert test set sequences to tensors\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For train and val test, we'll create a dataloaders which will pass batches of train data\n",
    "and validation data as input to the model during the training phase\n",
    "'''\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Before fine tuning, we'll freeze all the layers to prevent updating of model weights during fine tuning. However, we don't \n",
    "have to execute the code below if we wish to fine-tune the pre-trained weights of the BERT model.\n",
    "'''\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define architecture model\n",
    "'''\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "  def __init__(self, bert):\n",
    "    super(BERT_Arch, self).__init__()\n",
    "\n",
    "    self.bert = bert \n",
    "      \n",
    "    # dropout layer\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "    # relu activation function\n",
    "    self.relu =  nn.ReLU()\n",
    "\n",
    "    # dense layer 1\n",
    "    self.fc1 = nn.Linear(768,512)\n",
    "    \n",
    "    # dense layer 2 (Output layer)\n",
    "    self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "    #softmax activation function\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "  def forward(self, sent_id, mask):\n",
    "\n",
    "    #pass the inputs to the model  \n",
    "    _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict = False)\n",
    "    \n",
    "    x = self.fc1(cls_hs)\n",
    "\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    # output layer\n",
    "    x = self.fc2(x)\n",
    "    \n",
    "    # apply softmax activation\n",
    "    x = self.softmax(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  Modules   | Parameters |\n",
      "+------------+------------+\n",
      "| fc1.weight |   393216   |\n",
      "|  fc1.bias  |    512     |\n",
      "| fc2.weight |    1024    |\n",
      "|  fc2.bias  |     2      |\n",
      "+------------+------------+\n",
      "Total Trainable Params: 394754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "394754"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we can pass the pre-trained BERT to our architecture\n",
    "'''\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "#Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Once the model in our architecture, we can use the AdamW optimizer\n",
    "'''\n",
    "\n",
    "# optimizer from hugging face\n",
    "from transformers import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [2.10614525 0.65565217]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As our dataset is imbalanced (with majority of 'non dissident' labels) we'll first compute class \n",
    "weights for the labels in the train set and then pass the weights to the loss function in order to \n",
    "include the class imbalance in our model.\n",
    "'''\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute the class weights\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we convert our list of class to a tensor\n",
    "'''\n",
    "weights = torch.tensor(class_wts , dtype = torch.float)\n",
    "\n",
    "# push to GPU/CPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "cross_entropy = nn.NLLLoss(weight = weights)\n",
    "\n",
    "# Training epochs \n",
    "epochs = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have to define a couple of functions to train and evaluate the model\n",
    "'''\n",
    "\n",
    "# Train the model\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # For loop to iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progess update after every 50 batches\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(\"Batch {:> 5,} of {: > 5,}\".format(step, len(train_dataloader)))\n",
    "\n",
    "        # Push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To evaluate the model, our function will use the validation set data\n",
    "'''\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.5.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.694\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 2 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.694\n",
      "Validation Loss: 0.694\n",
      "\n",
      " Epoch 3 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.692\n",
      "Validation Loss: 0.695\n",
      "\n",
      " Epoch 4 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.690\n",
      "Validation Loss: 0.693\n",
      "\n",
      " Epoch 5 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.690\n",
      "Validation Loss: 0.692\n",
      "\n",
      " Epoch 6 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 7 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.689\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 8 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.690\n",
      "\n",
      " Epoch 9 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.690\n",
      "Validation Loss: 0.689\n",
      "\n",
      " Epoch 10 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.689\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 11 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.688\n",
      "Validation Loss: 0.688\n",
      "\n",
      " Epoch 12 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.686\n",
      "Validation Loss: 0.687\n",
      "\n",
      " Epoch 13 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.687\n",
      "Validation Loss: 0.690\n",
      "\n",
      " Epoch 14 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.687\n",
      "Validation Loss: 0.689\n",
      "\n",
      " Epoch 15 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.687\n",
      "Validation Loss: 0.687\n",
      "\n",
      " Epoch 16 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.687\n",
      "Validation Loss: 0.688\n",
      "\n",
      " Epoch 17 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.685\n",
      "Validation Loss: 0.688\n",
      "\n",
      " Epoch 18 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.685\n",
      "Validation Loss: 0.688\n",
      "\n",
      " Epoch 19 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.681\n",
      "Validation Loss: 0.688\n",
      "\n",
      " Epoch 20 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.686\n",
      "Validation Loss: 0.686\n",
      "\n",
      " Epoch 21 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.684\n",
      "Validation Loss: 0.685\n",
      "\n",
      " Epoch 22 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.684\n",
      "Validation Loss: 0.686\n",
      "\n",
      " Epoch 23 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.683\n",
      "Validation Loss: 0.690\n",
      "\n",
      " Epoch 24 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.682\n",
      "Validation Loss: 0.686\n",
      "\n",
      " Epoch 25 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.683\n",
      "Validation Loss: 0.686\n",
      "\n",
      " Epoch 26 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.682\n",
      "Validation Loss: 0.685\n",
      "\n",
      " Epoch 27 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.681\n",
      "Validation Loss: 0.684\n",
      "\n",
      " Epoch 28 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.683\n",
      "Validation Loss: 0.687\n",
      "\n",
      " Epoch 29 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.681\n",
      "Validation Loss: 0.684\n",
      "\n",
      " Epoch 30 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.679\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 31 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.683\n",
      "Validation Loss: 0.684\n",
      "\n",
      " Epoch 32 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.680\n",
      "Validation Loss: 0.684\n",
      "\n",
      " Epoch 33 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.680\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 34 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.676\n",
      "Validation Loss: 0.684\n",
      "\n",
      " Epoch 35 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.679\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 36 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.678\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 37 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.675\n",
      "Validation Loss: 0.680\n",
      "\n",
      " Epoch 38 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.680\n",
      "Validation Loss: 0.682\n",
      "\n",
      " Epoch 39 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.677\n",
      "Validation Loss: 0.681\n",
      "\n",
      " Epoch 40 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.677\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 41 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.680\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 42 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.678\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 43 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.673\n",
      "Validation Loss: 0.681\n",
      "\n",
      " Epoch 44 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.676\n",
      "Validation Loss: 0.681\n",
      "\n",
      " Epoch 45 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.674\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 46 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.673\n",
      "Validation Loss: 0.678\n",
      "\n",
      " Epoch 47 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.676\n",
      "Validation Loss: 0.678\n",
      "\n",
      " Epoch 48 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.676\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 49 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.670\n",
      "Validation Loss: 0.680\n",
      "\n",
      " Epoch 50 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.673\n",
      "Validation Loss: 0.680\n",
      "\n",
      " Epoch 51 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.675\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 52 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.672\n",
      "Validation Loss: 0.677\n",
      "\n",
      " Epoch 53 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.672\n",
      "Validation Loss: 0.677\n",
      "\n",
      " Epoch 54 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.673\n",
      "Validation Loss: 0.675\n",
      "\n",
      " Epoch 55 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.675\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 56 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.670\n",
      "Validation Loss: 0.677\n",
      "\n",
      " Epoch 57 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.673\n",
      "Validation Loss: 0.679\n",
      "\n",
      " Epoch 58 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.672\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 59 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.669\n",
      "Validation Loss: 0.676\n",
      "\n",
      " Epoch 60 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.671\n",
      "Validation Loss: 0.675\n",
      "\n",
      " Epoch 61 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.670\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 62 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.670\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 63 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.668\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 64 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.668\n",
      "Validation Loss: 0.675\n",
      "\n",
      " Epoch 65 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.668\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 66 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.669\n",
      "Validation Loss: 0.673\n",
      "\n",
      " Epoch 67 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.670\n",
      "Validation Loss: 0.676\n",
      "\n",
      " Epoch 68 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.664\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 69 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.668\n",
      "Validation Loss: 0.672\n",
      "\n",
      " Epoch 70 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.666\n",
      "Validation Loss: 0.673\n",
      "\n",
      " Epoch 71 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.667\n",
      "Validation Loss: 0.673\n",
      "\n",
      " Epoch 72 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.669\n",
      "Validation Loss: 0.670\n",
      "\n",
      " Epoch 73 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.667\n",
      "Validation Loss: 0.670\n",
      "\n",
      " Epoch 74 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.667\n",
      "Validation Loss: 0.673\n",
      "\n",
      " Epoch 75 / 75\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.668\n",
      "Validation Loss: 0.671\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After evaluating the model, we can fine-tuning it\n",
    "'''\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_losses)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.70      0.49        23\n",
      "           1       0.88      0.65      0.75        75\n",
      "\n",
      "    accuracy                           0.66        98\n",
      "   macro avg       0.63      0.67      0.62        98\n",
      "weighted avg       0.76      0.66      0.69        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "\n",
    "# load weights of the best model\n",
    "\n",
    "path = \"saved_weights.pt\" #\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# Model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45837b37786a7c8d5338cd4b969f6e58977db88c9afb8c608efcd8334ca060fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
