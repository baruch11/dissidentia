{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>charles</th>\n",
       "      <th>amir</th>\n",
       "      <th>moindze</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>261</td>\n",
       "      <td>les services publics disparaissent ou ouvrent ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>262</td>\n",
       "      <td>TRop de fonctionnaires, notamment a Education ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>263</td>\n",
       "      <td>Pas de privilèges ni avantages post mandats (p...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>264</td>\n",
       "      <td>Au total, l'Etat pourrait économiser près de 2...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>265</td>\n",
       "      <td>arrêter la dématérialisation à outrance</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text        charles  \\\n",
       "0   166  Pourtant, avec la non prise en compte du vote ...  non dissident   \n",
       "1   167            Supprimer toutes celles ne servant pas.  non dissident   \n",
       "2   168  suppression de l'ISF mal comprise/mal expliqué...  non dissident   \n",
       "3   169  Trop de ministres, trop de secrétaires d'état,...      dissident   \n",
       "4   170  Le système paraît trop lourd, coûteux et sourc...  non dissident   \n",
       "..  ...                                                ...            ...   \n",
       "95  261  les services publics disparaissent ou ouvrent ...  non dissident   \n",
       "96  262  TRop de fonctionnaires, notamment a Education ...  non dissident   \n",
       "97  263  Pas de privilèges ni avantages post mandats (p...  non dissident   \n",
       "98  264  Au total, l'Etat pourrait économiser près de 2...  non dissident   \n",
       "99  265            arrêter la dématérialisation à outrance  non dissident   \n",
       "\n",
       "             amir        moindze          final  \n",
       "0   non dissident  non dissident  non dissident  \n",
       "1       dissident      dissident      dissident  \n",
       "2   non dissident  non dissident  non dissident  \n",
       "3   non dissident      dissident      dissident  \n",
       "4   non dissident  non dissident  non dissident  \n",
       "..            ...            ...            ...  \n",
       "95  non dissident  non dissident  non dissident  \n",
       "96  non dissident  non dissident  non dissident  \n",
       "97  non dissident  non dissident  non dissident  \n",
       "98  non dissident  non dissident  non dissident  \n",
       "99  non dissident  non dissident  non dissident  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    " \n",
    "dataset = pd.read_csv(\"labels_v2.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>charles</th>\n",
       "      <th>amir</th>\n",
       "      <th>moindze</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>261</td>\n",
       "      <td>les services publics disparaissent ou ouvrent ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>262</td>\n",
       "      <td>TRop de fonctionnaires, notamment a Education ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>263</td>\n",
       "      <td>Pas de privilèges ni avantages post mandats (p...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>264</td>\n",
       "      <td>Au total, l'Etat pourrait économiser près de 2...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>265</td>\n",
       "      <td>arrêter la dématérialisation à outrance</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text        charles  \\\n",
       "0   166  Pourtant, avec la non prise en compte du vote ...  non dissident   \n",
       "1   167            Supprimer toutes celles ne servant pas.  non dissident   \n",
       "2   168  suppression de l'ISF mal comprise/mal expliqué...  non dissident   \n",
       "3   169  Trop de ministres, trop de secrétaires d'état,...      dissident   \n",
       "4   170  Le système paraît trop lourd, coûteux et sourc...  non dissident   \n",
       "..  ...                                                ...            ...   \n",
       "95  261  les services publics disparaissent ou ouvrent ...  non dissident   \n",
       "96  262  TRop de fonctionnaires, notamment a Education ...  non dissident   \n",
       "97  263  Pas de privilèges ni avantages post mandats (p...  non dissident   \n",
       "98  264  Au total, l'Etat pourrait économiser près de 2...  non dissident   \n",
       "99  265            arrêter la dématérialisation à outrance  non dissident   \n",
       "\n",
       "             amir        moindze          final  \n",
       "0   non dissident  non dissident  non dissident  \n",
       "1       dissident      dissident      dissident  \n",
       "2   non dissident  non dissident  non dissident  \n",
       "3   non dissident      dissident      dissident  \n",
       "4   non dissident  non dissident  non dissident  \n",
       "..            ...            ...            ...  \n",
       "95  non dissident  non dissident  non dissident  \n",
       "96  non dissident  non dissident  non dissident  \n",
       "97  non dissident  non dissident  non dissident  \n",
       "98  non dissident  non dissident  non dissident  \n",
       "99  non dissident  non dissident  non dissident  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename dataset columns\n",
    "\n",
    "dataset =dataset.rename(columns = {\"Category\" : 'label'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns and keep \"final\" and 'text' columns\n",
    "\n",
    "dataset = dataset.drop([\"id\",'charles', \"amir\", \"moindze\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Pourtant, avec la non prise en compte du vote ...      2\n",
       "1            Supprimer toutes celles ne servant pas.      0\n",
       "2  suppression de l'ISF mal comprise/mal expliqué...      2\n",
       "3  Trop de ministres, trop de secrétaires d'état,...      0\n",
       "4  Le système paraît trop lourd, coûteux et sourc...      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns \n",
    "\n",
    "dataset =dataset.rename(columns = {\"final\" : 'label'})\n",
    "dataset['label'] = dataset['label'].astype(\"category\")\n",
    "dataset.dtypes\n",
    "dataset['label'] = dataset['label'].cat.codes\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset into train, val, and test\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['text'], dataset['label'],\n",
    "                                                                    random_state = 2000,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    stratify = dataset['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                random_state = 2000,\n",
    "                                                                test_size = 0.4,\n",
    "                                                                stratify = temp_labels)\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import bert\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer, we'll use it to incode a couple of sentences using the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 10722, 3406, 102, 0, 0], [101, 2057, 1005, 2222, 2986, 8525, 2638, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#Sample data\n",
    "\n",
    "text = ['This is a bert model tuto', \"we'll finetune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding = True)\n",
    "\n",
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\\nWe add them to both sequences, and 0 represents the padding token.\\n\\n'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\n",
    "We add them to both sequences, and 0 represents the padding token.\n",
    "\n",
    "'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the sentences.\n",
    "\n",
    "'''\n",
    "With sentences of varying length, we'll use padding to make all the messages havethe same length\n",
    "Before, we'll display the distribution of the sequence lengths in the train set to find the right padding length\n",
    "'''\n",
    "sequence_length = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(sequence_length).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set 30 as the padding length\n",
    "'''\n",
    "\n",
    "# Tokenize & encode sequences\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 13,\n",
    "    padding = True,\n",
    "    truncation = True\n",
    ")\n",
    "\n",
    "# Tokenize and encode sequences in the validation set\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 13,\n",
    "    padding = True,\n",
    "    truncation = True\n",
    ")\n",
    "\n",
    "# Tokenize & encode sequences in the test set\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 13,\n",
    "    padding = True,\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we can convert sequences to tensors\n",
    "'''\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "\n",
    "# Convert validation set sequences to tensors\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Convert test set sequences to tensors\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For train and val test, we'll create a dataloaders which will pass batches of train data\n",
    "and validation data as input to the model during the training phase\n",
    "'''\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# Sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# Dataloader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# Sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# Dataloader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Before fine tuning, we'll freeze all the layers to prevent updating of model weights during fine tuning. However, we don't \n",
    "have to execute the code below if we wish to fine-tune the pre-trained weights of the BERT model.\n",
    "'''\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define architecture model\n",
    "'''\n",
    "\n",
    "class BERT_architecture(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT_architecture, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # ReLu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Dense layer 1\n",
    "        self.layer1 = nn.Linear(768, 512)\n",
    "        \n",
    "        # Dense layer 2\n",
    "        self.layer2 = nn.Linear(512, 2)\n",
    "\n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        # Give the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask = mask)\n",
    "\n",
    "        x = self.layer1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        # Softmax activation function\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\data\\baseline_transfer_learning.ipynb Cellule 18\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m BERT_architecture(bert)\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Push the model to GPU\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    984\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    983\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    984\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we can pass the pre-trained BERT to our architecture\n",
    "'''\n",
    "\n",
    "model = BERT_architecture(bert)\n",
    "\n",
    "#Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Once the model in our architecture, we can use the AdamW optimizer\n",
    "'''\n",
    "\n",
    "# optimizer from hugging face\n",
    "from transformers import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [2.22222222 4.44444444 0.43010753]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As our dataset is imbalanced (with majority of 'non dissident' labels) we'll first compute class \n",
    "weights for the labels in the train set and then pass the weights to the loss function in order to \n",
    "include the class imbalance in our model.\n",
    "'''\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute the class weights\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we convert our list of class to a tensor\n",
    "'''\n",
    "weights = torch.tensor(class_weights, dtype = torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weight = weights.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "cross_entropy = nn.NLLLoss(weight = weights)\n",
    "\n",
    "# Training epochs \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have to define a couple of functions to train and evaluate the model\n",
    "'''\n",
    "\n",
    "# Train the model\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # For loop to iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Make an update every 25 bathes\n",
    "        if step % 25 == 0 and not step == 0:\n",
    "            print(\"Batch {:5,} of {:>5,}\".format(step, len(train_dataloader)))\n",
    "\n",
    "    # Push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # Clear previously calculated gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # get model predictions for current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To evaluate the model, our function will use the validation set data\n",
    "'''\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"Evaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\data\\baseline_transfer_learning.ipynb Cellule 25\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_loss, _ \u001b[39m=\u001b[39m train()\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m valid_loss, _ \u001b[39m=\u001b[39m evaluate()\n",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\data\\baseline_transfer_learning.ipynb Cellule 25\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{:5,}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{:>5,}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(step, \u001b[39mlen\u001b[39m(train_dataloader)))\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Push the batch to gpu\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m batch \u001b[39m=\u001b[39m [r\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m sent_id, mask, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Clear previously calculated gradients\u001b[39;00m\n",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\data\\baseline_transfer_learning.ipynb Cellule 25\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{:5,}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{:>5,}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(step, \u001b[39mlen\u001b[39m(train_dataloader)))\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Push the batch to gpu\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m batch \u001b[39m=\u001b[39m [r\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m sent_id, mask, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Clear previously calculated gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After evaluating the model, we can fine-tuning it\n",
    "'''\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_weights.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\data\\baseline_transfer_learning.ipynb Cellule 26\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Make Predictions\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# load weights of the best model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msaved_weights.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Get predictions for test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/data/baseline_transfer_learning.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights.pt'"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "\n",
    "# load weights of the best model\n",
    "\n",
    "path = \"saved_weights.pt\" #\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# Model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45837b37786a7c8d5338cd4b969f6e58977db88c9afb8c608efcd8334ca060fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
