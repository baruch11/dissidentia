{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# Specifying device (CPU if GPU not compatible with CUDA)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>charles</th>\n",
       "      <th>amir</th>\n",
       "      <th>moindze</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>261</td>\n",
       "      <td>les services publics disparaissent ou ouvrent ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>262</td>\n",
       "      <td>TRop de fonctionnaires, notamment a Education ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>263</td>\n",
       "      <td>Pas de privilèges ni avantages post mandats (p...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>264</td>\n",
       "      <td>Au total, l'Etat pourrait économiser près de 2...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>265</td>\n",
       "      <td>arrêter la dématérialisation à outrance</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text        charles  \\\n",
       "0   166  Pourtant, avec la non prise en compte du vote ...  non dissident   \n",
       "1   167            Supprimer toutes celles ne servant pas.  non dissident   \n",
       "2   168  suppression de l'ISF mal comprise/mal expliqué...  non dissident   \n",
       "3   169  Trop de ministres, trop de secrétaires d'état,...      dissident   \n",
       "4   170  Le système paraît trop lourd, coûteux et sourc...  non dissident   \n",
       "..  ...                                                ...            ...   \n",
       "95  261  les services publics disparaissent ou ouvrent ...  non dissident   \n",
       "96  262  TRop de fonctionnaires, notamment a Education ...  non dissident   \n",
       "97  263  Pas de privilèges ni avantages post mandats (p...  non dissident   \n",
       "98  264  Au total, l'Etat pourrait économiser près de 2...  non dissident   \n",
       "99  265            arrêter la dématérialisation à outrance  non dissident   \n",
       "\n",
       "             amir        moindze          final  \n",
       "0   non dissident  non dissident  non dissident  \n",
       "1       dissident      dissident      dissident  \n",
       "2   non dissident  non dissident  non dissident  \n",
       "3   non dissident      dissident      dissident  \n",
       "4   non dissident  non dissident  non dissident  \n",
       "..            ...            ...            ...  \n",
       "95  non dissident  non dissident  non dissident  \n",
       "96  non dissident  non dissident  non dissident  \n",
       "97  non dissident  non dissident  non dissident  \n",
       "98  non dissident  non dissident  non dissident  \n",
       "99  non dissident  non dissident  non dissident  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    " \n",
    "dataset = pd.read_csv(\"labels_v2.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>charles</th>\n",
       "      <th>amir</th>\n",
       "      <th>moindze</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>dissident</td>\n",
       "      <td>dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>261</td>\n",
       "      <td>les services publics disparaissent ou ouvrent ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>262</td>\n",
       "      <td>TRop de fonctionnaires, notamment a Education ...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>263</td>\n",
       "      <td>Pas de privilèges ni avantages post mandats (p...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>264</td>\n",
       "      <td>Au total, l'Etat pourrait économiser près de 2...</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>265</td>\n",
       "      <td>arrêter la dématérialisation à outrance</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "      <td>non dissident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text        charles  \\\n",
       "0   166  Pourtant, avec la non prise en compte du vote ...  non dissident   \n",
       "1   167            Supprimer toutes celles ne servant pas.  non dissident   \n",
       "2   168  suppression de l'ISF mal comprise/mal expliqué...  non dissident   \n",
       "3   169  Trop de ministres, trop de secrétaires d'état,...      dissident   \n",
       "4   170  Le système paraît trop lourd, coûteux et sourc...  non dissident   \n",
       "..  ...                                                ...            ...   \n",
       "95  261  les services publics disparaissent ou ouvrent ...  non dissident   \n",
       "96  262  TRop de fonctionnaires, notamment a Education ...  non dissident   \n",
       "97  263  Pas de privilèges ni avantages post mandats (p...  non dissident   \n",
       "98  264  Au total, l'Etat pourrait économiser près de 2...  non dissident   \n",
       "99  265            arrêter la dématérialisation à outrance  non dissident   \n",
       "\n",
       "             amir        moindze          final  \n",
       "0   non dissident  non dissident  non dissident  \n",
       "1       dissident      dissident      dissident  \n",
       "2   non dissident  non dissident  non dissident  \n",
       "3   non dissident      dissident      dissident  \n",
       "4   non dissident  non dissident  non dissident  \n",
       "..            ...            ...            ...  \n",
       "95  non dissident  non dissident  non dissident  \n",
       "96  non dissident  non dissident  non dissident  \n",
       "97  non dissident  non dissident  non dissident  \n",
       "98  non dissident  non dissident  non dissident  \n",
       "99  non dissident  non dissident  non dissident  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename dataset columns\n",
    "\n",
    "dataset =dataset.rename(columns = {\"Category\" : 'label'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns and keep \"final\" and 'text' columns\n",
    "\n",
    "dataset = dataset.drop([\"id\",'charles', \"amir\", \"moindze\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pourtant, avec la non prise en compte du vote ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supprimer toutes celles ne servant pas.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suppression de l'ISF mal comprise/mal expliqué...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trop de ministres, trop de secrétaires d'état,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le système paraît trop lourd, coûteux et sourc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Pourtant, avec la non prise en compte du vote ...      2\n",
       "1            Supprimer toutes celles ne servant pas.      0\n",
       "2  suppression de l'ISF mal comprise/mal expliqué...      2\n",
       "3  Trop de ministres, trop de secrétaires d'état,...      0\n",
       "4  Le système paraît trop lourd, coûteux et sourc...      2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns \n",
    "\n",
    "dataset =dataset.rename(columns = {\"final\" : 'label'})\n",
    "dataset['label'] = dataset['label'].astype(\"category\")\n",
    "dataset.dtypes\n",
    "dataset['label'] = dataset['label'].cat.codes\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset into train, val, and test\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['text'], dataset['label'],\n",
    "                                                                    random_state = 2000,\n",
    "                                                                    test_size = 0.3,\n",
    "                                                                    stratify = dataset['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                random_state = 2000,\n",
    "                                                                test_size = 0.3,\n",
    "                                                                stratify = temp_labels)\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import bert-based pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer, we'll use it to incode a couple of sentences using the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8292, 6895, 9765, 4895, 2944, 2063, 14324, 102, 0, 0, 0, 0, 0, 0], [101, 2053, 2271, 2035, 5644, 21183, 24411, 2121, 8292, 2944, 2063, 10364, 10289, 5579, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#Sample data\n",
    "\n",
    "text = ['Ceci est un modele BERT', \"nous allons utiliser ce modele pour notre classification\"]\n",
    "\n",
    "# encode text\n",
    "\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding = True, truncation = True)\n",
    "\n",
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\\nWe add them to both sequences, and 0 represents the padding token.\\n\\n'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_ds : contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens.\n",
    "We add them to both sequences, and 0 represents the padding token.\n",
    "\n",
    "'attention mask' contains the 0's and 1's. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the sentences.\n",
    "\n",
    "'''\n",
    "With sentences of varying length, we'll use padding to make all the messages havethe same length\n",
    "Before, we'll display the distribution of the sequence lengths in the train set to find the right padding length\n",
    "'''\n",
    "sequence_length = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(sequence_length).hist(bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Set 30 as the padding length\n",
    "'''\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we can convert sequences to tensors\n",
    "'''\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "\n",
    "# Convert validation set sequences to tensors\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Convert test set sequences to tensors\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For train and val test, we'll create a dataloaders which will pass batches of train data\n",
    "and validation data as input to the model during the training phase\n",
    "'''\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Before fine tuning, we'll freeze all the layers to prevent updating of model weights during fine tuning. However, we don't \n",
    "have to execute the code below if we wish to fine-tune the pre-trained weights of the BERT model.\n",
    "'''\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define architecture model\n",
    "'''\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "  def __init__(self, bert):\n",
    "    super(BERT_Arch, self).__init__()\n",
    "\n",
    "    self.bert = bert \n",
    "      \n",
    "    # dropout layer\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "    # relu activation function\n",
    "    self.relu =  nn.ReLU()\n",
    "\n",
    "    # dense layer 1\n",
    "    self.fc1 = nn.Linear(768,512)\n",
    "    \n",
    "    # dense layer 2 (Output layer)\n",
    "    self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "    #softmax activation function\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we can pass the pre-trained BERT to our architecture\n",
    "'''\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "#Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Once the model in our architecture, we can use the AdamW optimizer\n",
    "'''\n",
    "\n",
    "# optimizer from hugging face\n",
    "from transformers import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [2.33333333 3.88888889 0.43209877]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As our dataset is imbalanced (with majority of 'non dissident' labels) we'll first compute class \n",
    "weights for the labels in the train set and then pass the weights to the loss function in order to \n",
    "include the class imbalance in our model.\n",
    "'''\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute the class weights\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we convert our list of class to a tensor\n",
    "'''\n",
    "weights = torch.tensor(class_wts , dtype = torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "cross_entropy = nn.NLLLoss(weight = weights)\n",
    "\n",
    "# Training epochs \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have to define a couple of functions to train and evaluate the model\n",
    "'''\n",
    "\n",
    "# Train the model\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # For loop to iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progess update after every 50 batches\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(\"Batch {:> 5,} of {: > 5,}\".format(step, len(train_dataloader)))\n",
    "\n",
    "        # Push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To evaluate the model, our function will use the validation set data\n",
    "'''\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [BERT_Arch] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\notebooks\\baseline_transfer_learning.ipynb Cellule 25\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_loss, _ \u001b[39m=\u001b[39m train()\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m valid_loss, _ \u001b[39m=\u001b[39m evaluate()\n",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\notebooks\\baseline_transfer_learning.ipynb Cellule 25\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# get model predictions for current batch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m preds \u001b[39m=\u001b[39m model(sent_id, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# compute the loss between actual and predicted values\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy(preds, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:244\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [BERT_Arch] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After evaluating the model, we can fine-tuning it\n",
    "'''\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_losses)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_weights.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\amir\\projet2_nlp\\dissidentia\\notebooks\\baseline_transfer_learning.ipynb Cellule 26\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Make Predictions\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# load weights of the best model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msaved_weights.pt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Get predictions for test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/amir/projet2_nlp/dissidentia/notebooks/baseline_transfer_learning.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights.pt'"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "\n",
    "# load weights of the best model\n",
    "\n",
    "path = \"saved_weights.pt\" #\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# Model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45837b37786a7c8d5338cd4b969f6e58977db88c9afb8c608efcd8334ca060fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
